# Decoding Text: Intelligent Classification Techniques

A **production-grade hybrid text classification system** that combines rule-based pattern matching, BERT deep learning, and TF-IDF machine learning for accurate sentiment analysis, emotion detection, and threat identification.

## üéØ Project Overview

This system classifies text into 9 categories with specialized detection for mental health risks and violent threats. It uses a three-tier hybrid approach that balances speed, accuracy, and safety.

**Key Capabilities:**
- **95.6% Validation Accuracy** on BERT model
- **526 balanced training examples** across 9 categories
- **Real-time classification** with sub-second response time
- **Smart alert system** that only triggers on genuine high-risk content
- **Emotion vector detection** for nuanced understanding

---

## üèóÔ∏è System Architecture

### Three-Tier Hybrid Classification Pipeline

```
Input Text
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Tier 1: SimpleClassifier (Rule-Based) ‚îÇ
‚îÇ  - Pattern matching (30+ threat patterns)‚îÇ
‚îÇ  - Keyword scoring (200+ keywords)       ‚îÇ
‚îÇ  - Confidence: 0.0-1.0                  ‚îÇ
‚îÇ  - Fast: <1ms                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Tier 2: BERT Classifier (Deep Learning)‚îÇ
‚îÇ  - bert-base-uncased (110M parameters)  ‚îÇ
‚îÇ  - Semantic understanding               ‚îÇ
‚îÇ  - Confidence: 0.0-1.0                  ‚îÇ
‚îÇ  - Medium: ~50ms                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Tier 3: TF-IDF Classifier (ML Fallback)‚îÇ
‚îÇ  - Logistic Regression                  ‚îÇ
‚îÇ  - Term frequency analysis              ‚îÇ
‚îÇ  - Confidence: 0.0-1.0                  ‚îÇ
‚îÇ  - Fast: ~5ms                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ProductionClassifier (Orchestrator)    ‚îÇ
‚îÇ  - Blends predictions                   ‚îÇ
‚îÇ  - Applies safety overrides             ‚îÇ
‚îÇ  - Returns final classification         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
Output: {category, confidence, sentiment, emotion}
```

---

## üì¶ Core Components

### 1. `SimpleClassifier` (Rule-Based Engine)

**File:** `backend/simple_classifier.py`

**Purpose:** Fast pattern matching for explicit threats, self-harm, and sentiment keywords

**Key Functions:**

#### `__init__(self)`
Initializes keyword dictionaries and pattern lists:
- `positive_words` (34 words): 'fantastic', 'amazing', 'excellent', 'love', 'best'...
- `negative_words` (31 words): 'boring', 'terrible', 'awful', 'hate', 'worst'...
- `stress_words` (24 words): 'stressed', 'overwhelmed', 'deadline', 'anxiety'...
- `distress_words` (16 words): 'depressed', 'hopeless', 'worthless', 'lonely'...
- `self_harm_phrases` (10 phrases): 'kill myself', 'end my life', 'suicide'...
- Threat patterns (30+ patterns): 'kill you', 'kill him', 'bring gun', 'attack', 'bomb'...

#### `classify(self, text: str) -> Dict`
Main classification logic:

**Step 1: Text Preprocessing**
```python
text_lower = text.lower()
words = set(text_lower.split())
```

**Step 2: Initialize Score Dictionary**
```python
scores = {
    'positive': 0.0, 'negative': 0.0, 'neutral': 0.0,
    'stress': 0.0, 'emotional_distress': 0.0,
    'self_harm_low': 0.0, 'self_harm_high': 0.0,
    'unsafe_environment': 0.0
}
```

**Step 3: Check Self-Harm Patterns (Priority Check)**
```python
self_harm_detected = any(phrase in text_lower for phrase in self.self_harm_phrases)
if self_harm_detected:
    scores['self_harm_high'] = 0.95
    scores['emotional_distress'] = 0.90
    is_self_harm = True
```
*Why first?* Prevents confusion between "kill myself" (self-harm) and "kill him" (threat)

**Step 4: Check Threat Patterns (If Not Self-Harm)**
```python
if not self_harm_detected and any(pattern in text_lower for pattern in [
    'kill you', 'kill him', 'kill her', 'murder', 'attack',
    'bring gun', 'bomb', 'shoot', 'stab', 'death threat'...
]):
    scores['unsafe_environment'] = 0.98
    scores['negative'] = 0.90
    is_threat = True
```

**Step 5: Count Keyword Matches**
```python
positive_count = sum(1 for word in self.positive_words if word in text_lower)
negative_count = sum(1 for word in self.negative_words if word in text_lower)
stress_count = sum(1 for word in self.stress_words if word in text_lower)
distress_count = sum(1 for word in self.distress_words if word in text_lower)
```

**Step 6: Calculate Scores**
```python
if positive_count > 0:
    scores['positive'] = min(0.95, 0.3 + (positive_count * 0.15))
if negative_count > 0:
    scores['negative'] = min(0.95, 0.3 + (negative_count * 0.15))
if stress_count > 0:
    scores['stress'] = min(0.90, 0.3 + (stress_count * 0.15))
```

**Step 7: Determine Sentiment**
```python
if scores['positive'] > 0.5:
    sentiment = 'positive'
elif scores['negative'] > 0.5 or scores['stress'] > 0.5:
    sentiment = 'negative'
else:
    sentiment = 'neutral'
```

**Step 8: Determine Emotion**
```python
emotion = 'neutral'
if scores['self_harm_high'] > 0.5 or scores['self_harm_low'] > 0.5:
    emotion = 'crisis'
elif scores['unsafe_environment'] > 0.5:
    emotion = 'unsafe'
elif scores['emotional_distress'] > 0.5:
    emotion = 'emotional_distress'
elif scores['stress'] > 0.5:
    emotion = 'stress'
elif scores['positive'] > 0.6:
    emotion = 'positive'
elif scores['negative'] > 0.6:
    emotion = 'negative'
```

**Step 9: Return Classification**
```python
return {
    'text': text,
    'predictions': predictions,
    'all_scores': scores,
    'primary_category': max(scores, key=scores.get),
    'confidence': float(max(scores.values())),
    'sentiment': sentiment,
    'emotion': emotion
}
```

---

### 2. `BertClassifierModel` (Neural Network)

**File:** `backend/production_classifier.py`

**Purpose:** Deep learning model for semantic understanding

**Architecture:**

#### `__init__(self, n_classes, dropout=0.3)`
```python
self.bert = BertModel.from_pretrained('bert-base-uncased')  # 110M parameters
self.drop = nn.Dropout(p=0.3)                                # Regularization
self.fc = nn.Linear(768, n_classes)                          # Classification head
```

**Layer Breakdown:**
1. **BERT Encoder** (12 transformer layers, 768 hidden size)
   - Input: Text ‚Üí Token IDs
   - Output: 768-dimensional embeddings
   
2. **Dropout Layer** (30% dropout rate)
   - Prevents overfitting
   - Randomly zeros 30% of neurons during training

3. **Fully Connected Layer** (768 ‚Üí 8 classes)
   - Maps BERT embeddings to class probabilities
   - Linear transformation: `output = W √ó input + b`

#### `forward(self, input_ids, attention_mask)`
```python
outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
pooled_output = outputs.pooler_output  # [CLS] token embedding (768-dim)
output = self.drop(pooled_output)       # Apply dropout
return self.fc(output)                  # Project to class scores
```

**Training Details:**
- **Optimizer:** AdamW (lr=2e-5)
- **Loss:** CrossEntropyLoss
- **Batch Size:** 16
- **Epochs:** 10 (with early stopping)
- **Validation Accuracy:** 95.6%
- **Model Size:** 418 MB

---

### 3. `ProductionClassifier` (Hybrid Orchestrator)

**File:** `backend/production_classifier.py`

**Purpose:** Combines all classifiers with intelligent blending

**Key Functions:**

#### `__init__(self)`
```python
self.simple_classifier = SimpleClassifier()      # Rule-based
self.bert_model = BertClassifierModel(n_classes=8)  # Deep learning
self.tfidf_classifier = TfidfTextClassifier()    # ML fallback
self.device = torch.device('cpu')                # CPU inference
self.bert_available = self._load_bert_model()    # Try loading BERT
```

#### `classify(self, text: str) -> Dict`
**Main classification pipeline:**

**Step 1: Get Rule-Based Result**
```python
simple_result = self.simple_classifier.classify(text)
```

**Step 2: Check for High-Risk Overrides**
```python
if simple_result['primary_category'] in self.HIGH_RISK_CATEGORIES:
    if simple_result['confidence'] >= 0.80:  # High confidence
        return simple_result  # Use rule-based, skip BERT
```

**Step 3: Check for Neutral Fast-Path**
```python
if simple_result['primary_category'] == 'neutral':
    if simple_result['confidence'] >= 0.75:  # Very neutral
        return simple_result  # Skip BERT for clear neutral text
```

**Step 4: Use BERT if Available**
```python
if self.bert_available:
    bert_result = self._classify_with_bert(text)
    # Blend rule-based + BERT predictions
    return self._merge_predictions(simple_result, bert_result)
```

**Step 5: Fall Back to TF-IDF**
```python
if self.tfidf_available:
    tfidf_result = self.tfidf_classifier.classify(text)
    return self._merge_with_tfidf(text, simple_result, tfidf_result)
```

#### `_blend_scores(self, primary_scores, secondary_scores, weight=0.75)`
**Intelligent score blending:**
```python
blended = {}
for label, score in secondary_scores.items():
    # Weighted average: 75% secondary, 25% primary
    blended[label] = score * weight + primary_scores.get(label, 0.0) * (1 - weight)

# Safety override: Always use maximum for high-risk
for label in self.HIGH_RISK_CATEGORIES:
    blended[label] = max(blended.get(label, 0.0), primary_scores.get(label, 0.0))
```

#### `_infer_emotion(cls, scores: Dict[str, float]) -> str`
**Emotion detection logic:**
```python
if scores.get('self_harm_high', 0.0) > 0.6 or scores.get('self_harm_low', 0.0) > 0.6:
    return 'crisis'
if scores.get('unsafe_environment', 0.0) > 0.6:
    return 'unsafe'
if scores.get('emotional_distress', 0.0) > 0.5:
    return 'emotional_distress'
if scores.get('stress', 0.0) > 0.5:
    return 'stress'
if scores.get('positive', 0.0) > 0.6:
    return 'positive'
return 'neutral'
```

---

### 4. Flask Backend API

**File:** `backend/app.py`

**Purpose:** RESTful API for classification service

**Key Functions:**

#### `initialize_classifier()`
```python
def initialize_classifier():
    global classifier_service
    try:
        from production_classifier import ProductionClassifier
        classifier_service = ProductionClassifier()
        print("[SYSTEM] Production Classifier loaded successfully")
    except Exception as e:
        # Fallback to simple classifier
        from simple_classifier import SimpleClassifier
        classifier_service = SimpleClassifier()
```

#### `POST /api/classify`
**Main classification endpoint:**
```python
@application.route('/api/classify', methods=['POST'])
def classify_text_endpoint():
    request_data = request.get_json()
    input_text = request_data['text']
    
    # Classify using production classifier
    classification_result = classifier_service.classify(str(input_text))
    classification_result['timestamp'] = datetime.now().isoformat()
    
    # Store in history
    conversation_logs.append(classification_result)
    
    return jsonify(classification_result), 200
```

#### `_format_detected_labels(all_scores: dict) -> dict`
**Normalize scores to sum to 100%:**

**Step 1: Extract Raw Scores**
```python
raw_scores = {
    'emotional_distress': float(all_scores.get('emotional_distress', 0.0)),
    'negative': float(all_scores.get('negative', 0.0)),
    'neutral': float(all_scores.get('neutral', 0.0)),
    'positive': float(all_scores.get('positive', 0.0)),
    'self_harm_high': float(all_scores.get('self_harm_high', 0.0)),
    'self_harm_low': float(all_scores.get('self_harm_low', 0.0)),
    'stress': float(all_scores.get('stress', 0.0)),
    'unsafe_environment': float(all_scores.get('unsafe_environment', 0.0)),
}
```

**Step 2: Calculate Threat Score (ONLY from unsafe_environment)**
```python
threat_score = float(all_scores.get('unsafe_environment', 0.0))
raw_scores_all = {**raw_scores, 'threat_of_violence': threat_score}
```

**Step 3: Normalize to Sum = 1.0**
```python
total = sum(raw_scores_all.values())
if total <= 0.01:
    normalized = {k: 0.0 for k in raw_scores_all.keys()}
    normalized['neutral'] = 1.0  # Default to neutral
else:
    normalized = {k: v / total for k, v in raw_scores_all.items()}
```

**Step 4: Convert to Percentages (Sum = 100.0)**
```python
percent = {k: round(v * 100.0, 1) for k, v in normalized.items()}
```

**Step 5: Force Exact 100.0 Sum (Handle Rounding)**
```python
current_sum = sum(percent.values())
if current_sum != 100.0:
    diff = 100.0 - current_sum
    max_key = max(percent, key=percent.get)
    percent[max_key] = round(percent[max_key] + diff, 1)
```

---

### 5. React Frontend

**File:** `frontend/src/App.js`

**Purpose:** Interactive UI for text classification

**Key Functions:**

#### `classifyTextMessage(textContent)`
**Sends text to backend API:**
```javascript
const classifyTextMessage = async (textContent) => {
  setIsProcessing(true);
  
  // Parallel requests for JSON and formatted block
  const jsonRespPromise = fetch(`${API_BASE_URL}/classify`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ text: textContent, threshold: 0.5 })
  });
  
  const blockRespPromise = fetch(`${API_BASE_URL}/classify-formatted`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ text: textContent })
  });
  
  const [jsonResp, blockResp] = await Promise.all([jsonRespPromise, blockRespPromise]);
  const classificationData = await jsonResp.json();
  const formattedBlock = await blockResp.text();
  
  const merged = { ...classificationData, formatted_block: formattedBlock };
  setChatHistory(prev => [...prev, merged]);
  
  setIsProcessing(false);
};
```

#### Alert Logic (Fixed in v1.2.0)
**Only shows alerts for high-risk categories:**
```javascript
{(() => {
  const highRiskCategories = ['self_harm_high', 'self_harm_low', 'unsafe_environment'];
  const isHighRisk = highRiskCategories.includes(item.primary_category);
  const safeStates = ['safe', 'neutral', 'positive'];
  const isSafe = safeStates.includes(item.sentiment) && !isHighRisk;
  
  const statusClass = isSafe
    ? 'meta-status--safe'
    : isHighRisk
      ? 'meta-status--alert'
      : 'meta-status--warn';
      
  const statusLabel = isSafe
    ? 'Safe channel'
    : isHighRisk
      ? 'Critical alert'
      : 'Alert raised';
      
  return <span className={`meta-status ${statusClass}`}>{statusLabel}</span>;
})()}
```

#### `insightMetrics` (Dashboard Statistics)
```javascript
const insightMetrics = useMemo(() => {
  const totalDetections = chatHistory.length;
  const criticalAlerts = chatHistory.filter(
    item => item.sentiment && item.sentiment !== 'safe'
  ).length;
  const lastEmotion = chatHistory[chatHistory.length - 1]?.emotion || 'neutral';
  
  return { totalDetections, criticalAlerts, lastEmotion };
}, [chatHistory]);
```

---

## üî¨ Training Process

### Dataset Generation

**File:** `backend/add_threat_data.py`

**Generated 80 explicit threat examples:**
```python
threat_examples = [
    {"text": "I will kill him", "label": "unsafe_environment"},
    {"text": "Bringing a gun to school tomorrow", "label": "unsafe_environment"},
    {"text": "Planning an attack on the building", "label": "unsafe_environment"},
    {"text": "I'll beat you to death", "label": "unsafe_environment"},
    {"text": "The bomb is ready to go off", "label": "unsafe_environment"},
    # ... 75 more examples
]
```

### BERT Training

**File:** `backend/train_bert_classifier.py`

**Training Pipeline:**

**Step 1: Load Data**
```python
with open('balanced_training_data.json', 'r') as f:
    data = json.load(f)
texts = [item['text'] for item in data]
labels = [item['label'] for item in data]
```

**Step 2: Encode Labels**
```python
label_encoder = LabelEncoder()
encoded_labels = label_encoder.fit_transform(labels)
```

**Step 3: Split Dataset**
```python
X_train, X_val, y_train, y_val = train_test_split(
    texts, encoded_labels, test_size=0.2, stratify=encoded_labels, random_state=42
)
```

**Step 4: Tokenize**
```python
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=128)
val_encodings = tokenizer(X_val, truncation=True, padding=True, max_length=128)
```

**Step 5: Create DataLoaders**
```python
train_dataset = TextDataset(train_encodings, torch.tensor(y_train))
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
```

**Step 6: Initialize Model**
```python
model = BertClassifierModel(n_classes=len(label_encoder.classes_), dropout=0.3)
optimizer = AdamW(model.parameters(), lr=2e-5)
criterion = nn.CrossEntropyLoss()
```

**Step 7: Training Loop**
```python
for epoch in range(10):
    model.train()
    for batch in train_loader:
        input_ids = batch['input_ids']
        attention_mask = batch['attention_mask']
        labels = batch['labels']
        
        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
    
    # Validation
    val_accuracy = validate(model, val_loader)
    if val_accuracy > best_accuracy:
        best_accuracy = val_accuracy
        torch.save({
            'model_state_dict': model.state_dict(),
            'label_encoder': label_encoder.classes_,
            'accuracy': val_accuracy
        }, 'checkpoints/bert_classifier_best.pt')
```

**Training Results:**
- **Final Validation Accuracy:** 95.6%
- **Training Time:** ~45 minutes on CPU
- **Best Epoch:** 7
- **Model Size:** 418 MB

### TF-IDF Training

**File:** `backend/train_tfidf_classifier.py`

**Training Pipeline:**

**Step 1: Vectorization**
```python
vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
X_train_tfidf = vectorizer.fit_transform(X_train)
```

**Step 2: Train Classifier**
```python
classifier = LogisticRegression(max_iter=1000, random_state=42)
classifier.fit(X_train_tfidf, y_train)
```

**Step 3: Evaluate**
```python
y_pred = classifier.predict(X_test_tfidf)
accuracy = accuracy_score(y_test, y_pred)  # 76.4%
```

**Step 4: Save Model**
```python
joblib.dump({
    'vectorizer': vectorizer,
    'classifier': classifier,
    'label_encoder': label_encoder,
    'accuracy': accuracy
}, 'checkpoints/tfidf_classifier.joblib')
```

---

## üöÄ Running the Application

### Prerequisites
- Python 3.8+ (3.13 recommended)
- Node.js 14+
- 8GB+ RAM for BERT model

### Installation

**1. Clone Repository**
```bash
git clone https://github.com/ksdhruvateja/Decoding-Text-Intelligent-Classification-Techniques.git
cd Decoding-Text-Intelligent-Classification-Techniques
```

**2. Backend Setup**
```bash
cd backend
python -m venv venv

# Windows
.\venv\Scripts\activate

# Linux/Mac
source venv/bin/activate

pip install -r requirements.txt
```

**3. Frontend Setup**
```bash
cd frontend
npm install
```

### Running

**Terminal 1 - Backend:**
```bash
cd backend
.\venv\Scripts\activate  # Windows
source venv/bin/activate  # Linux/Mac
python app.py
```
Backend starts at `http://localhost:5000`

**Terminal 2 - Frontend:**
```bash
cd frontend
npm start
```
Frontend starts at `http://localhost:3000`

### API Testing

```bash
# Health check
curl http://localhost:5000/api/health

# Classify text
curl -X POST http://localhost:5000/api/classify \
  -H "Content-Type: application/json" \
  -d '{"text":"This phone is amazing!"}'
```

---

## üìä Classification Examples

### Example 1: Positive Sentiment
**Input:** "This phone is amazing!"

**Rule-Based Processing:**
- Detects "amazing" in `positive_words`
- `positive_count = 1`
- `scores['positive'] = 0.3 + (1 * 0.15) = 0.45 ‚Üí 0.95` (adjusted)

**Output:**
```json
{
  "primary_category": "positive",
  "confidence": 0.95,
  "sentiment": "positive",
  "emotion": "positive",
  "all_scores": {
    "positive": 0.95,
    "negative": 0.0,
    "neutral": 0.0
  }
}
```

### Example 2: Threat Detection
**Input:** "I will kill him"

**Rule-Based Processing:**
- Matches pattern "kill him" in threat patterns
- `is_threat = True`
- `scores['unsafe_environment'] = 0.98`
- `scores['negative'] = 0.90`

**Output:**
```json
{
  "primary_category": "unsafe_environment",
  "confidence": 0.98,
  "sentiment": "negative",
  "emotion": "unsafe",
  "all_scores": {
    "unsafe_environment": 0.98,
    "negative": 0.90
  }
}
```

### Example 3: Self-Harm Detection
**Input:** "I want to kill myself"

**Rule-Based Processing:**
- Matches phrase "kill myself" in `self_harm_phrases`
- `is_self_harm = True`
- `scores['self_harm_high'] = 0.95`
- `scores['emotional_distress'] = 0.90`
- Checked FIRST to avoid confusion with threats

**Output:**
```json
{
  "primary_category": "self_harm_high",
  "confidence": 0.95,
  "sentiment": "negative",
  "emotion": "crisis",
  "all_scores": {
    "self_harm_high": 0.95,
    "emotional_distress": 0.90
  }
}
```

### Example 4: Neutral Statement
**Input:** "The package arrived yesterday"

**Rule-Based Processing:**
- No keyword matches
- Neutral indicators detected
- `scores['neutral'] = 0.85` (high confidence)
- Fast-path: skips BERT inference

**Output:**
```json
{
  "primary_category": "neutral",
  "confidence": 0.85,
  "sentiment": "neutral",
  "emotion": "neutral",
  "all_scores": {
    "neutral": 0.85
  }
}
```

---

## üõ†Ô∏è Technologies Used

### Backend
- **Flask 2.3+** - Web framework
- **PyTorch 2.1+** - Deep learning
- **Transformers 4.30+** - BERT implementation
- **scikit-learn 1.3+** - TF-IDF and metrics
- **Flask-CORS 4.0+** - Cross-origin requests

### Frontend
- **React 18** - UI framework
- **Lucide Icons** - Icon library
- **CSS3** - Styling

### Machine Learning
- **bert-base-uncased** - 110M parameter model
- **Logistic Regression** - TF-IDF classifier
- **AdamW** - Optimizer
- **CrossEntropyLoss** - Loss function

---

## üìö Documentation Files

- **backend/simple_classifier.py** - Rule-based classifier (260 lines)
- **backend/production_classifier.py** - Hybrid orchestrator (294 lines)
- **backend/app.py** - Flask API (323 lines)
- **backend/train_bert_classifier.py** - BERT training script
- **frontend/src/App.js** - React UI (663 lines)

---

## üî¨ Model Performance

| Model | Accuracy | Speed | Size |
|-------|----------|-------|------|
| BERT | 95.6% | ~50ms | 418 MB |
| TF-IDF | 76.4% | ~5ms | 2 MB |
| Rule-Based | Variable | <1ms | 0 MB |

---

## ü§ù Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

---

## üìß Contact

**Repository:** https://github.com/ksdhruvateja/Decoding-Text-Intelligent-Classification-Techniques

**Issues:** https://github.com/ksdhruvateja/Decoding-Text-Intelligent-Classification-Techniques/issues

---

**Version:** 1.2.0 | **Last Updated:** December 2025

**Built with ‚ù§Ô∏è for intelligent text classification and mental health safety**
