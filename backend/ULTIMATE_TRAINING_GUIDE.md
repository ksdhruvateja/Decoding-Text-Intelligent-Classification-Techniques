# Ultimate Training Guide - LLMs + DL + RL

## üéØ Overview

This guide covers the **ultimate training pipeline** that combines:
- ‚úÖ **LLM-based data generation** (GPT, Hugging Face models)
- ‚úÖ **Advanced Deep Learning** (BERT, RoBERTa, DeBERTa ensembles)
- ‚úÖ **Reinforcement Learning from Human Feedback (RLHF)**
- ‚úÖ **Self-training and active learning**
- ‚úÖ **Hyperparameter optimization**
- ‚úÖ **Ensemble methods**

**Target: 98%+ accuracy on all statement types**

## üöÄ Quick Start

### Option 1: Run Complete Pipeline (Recommended)

```bash
cd backend
python ultimate_training_pipeline.py
```

This will automatically:
1. Generate LLM-based training data
2. Train multiple DL architectures
3. Apply RLHF fine-tuning
4. Create ensemble models
5. Optimize hyperparameters

**Time**: 6-24 hours (depending on GPU)  
**Expected Improvement**: +20-30% accuracy

### Option 2: Run Individual Components

#### Step 1: Generate LLM Data

```bash
python llm_data_generator.py
```

**Requirements:**
- OpenAI API key (optional, for GPT-4 generation)
- Or uses Hugging Face models (free)

**Output**: `llm_generated_training_data.json`

#### Step 2: Train Advanced DL Models

```bash
python advanced_dl_trainer.py
```

**Trains:**
- BERT with attention pooling
- RoBERTa
- DeBERTa (if available)

**Output**: `checkpoints/advanced_bert_model.pt`, etc.

#### Step 3: RLHF Fine-tuning

```bash
python rlhf_trainer.py
```

**Requirements:**
- `feedback_data.json` with human feedback

**Output**: `checkpoints/rlhf_finetuned_model.pt`

#### Step 4: Ensemble Training

```bash
python train_ensemble.py
```

**Combines:**
- Multiple architectures
- Weighted voting
- Best accuracy

**Output**: `checkpoints/ensemble_model.pt`

## üìã Detailed Components

### 1. LLM Data Generation (`llm_data_generator.py`)

**Purpose**: Generate diverse, high-quality training examples using LLMs

**Features:**
- GPT-4/GPT-3.5 for high-quality generation
- Hugging Face models for free alternative
- Paraphrasing for data augmentation
- Category-specific generation

**Usage:**
```python
from llm_data_generator import LLMDataGenerator

generator = LLMDataGenerator(use_openai=True, use_hf=True)
examples = generator.generate_comprehensive_dataset(target_size=2000)
```

**Configuration:**
- Set `OPENAI_API_KEY` environment variable for GPT
- Adjust `target_size` for desired dataset size
- Modify categories/emotions as needed

### 2. Advanced DL Training (`advanced_dl_trainer.py`)

**Purpose**: Train multiple state-of-the-art architectures

**Architectures:**
- **BERT**: Bidirectional Encoder Representations
- **RoBERTa**: Robustly Optimized BERT
- **DeBERTa**: Decoding-enhanced BERT

**Techniques:**
- Attention pooling
- Focal loss for imbalanced classes
- Mixed precision training (FP16)
- Cosine annealing LR schedule
- Early stopping
- Gradient clipping

**Configuration:**
```python
config = {
    'batch_size': 16,
    'epochs': 15,
    'learning_rate': 2e-5,
    'weight_decay': 0.01,
    'dropout': 0.3,
    'focal_gamma': 2.0,
    'warmup_ratio': 0.1,
    'loss': 'focal',
    'scheduler': 'cosine',
    'mixed_precision': True,
    'patience': 5
}
```

### 3. RLHF Training (`rlhf_trainer.py`)

**Purpose**: Fine-tune model using human feedback signals

**How it works:**
1. Collect predictions on validation set
2. Get human feedback (correct/incorrect)
3. Compute reward signal
4. Fine-tune model to maximize reward

**Feedback Data Format:**
```json
[
  {
    "text": "example text",
    "true_labels": {"neutral": 1, "stress": 0, ...},
    "predicted_labels": {"neutral": 0.9, "stress": 0.1, ...},
    "feedback": 1.0
  }
]
```

**Loss Function:**
- Combines classification loss (BCE) with reward signal
- Reward = feedback * 0.7 + accuracy * 0.3
- Model learns to maximize reward

### 4. Ensemble Training (`train_ensemble.py`)

**Purpose**: Combine multiple models for best accuracy

**Methods:**
- Weighted voting (by F1 score)
- Average probabilities
- Stacking (meta-learner)

**Expected Improvement**: +2-5% over single best model

### 5. Hyperparameter Optimization (`hyperparameter_optimization.py`)

**Purpose**: Automatically find best hyperparameters

**Method**: Optuna (Bayesian optimization)

**Optimizes:**
- Learning rate
- Batch size
- Dropout
- Focal loss gamma
- Weight decay
- Warmup ratio

**Time**: 6-12 hours for 50 trials

## üîß Configuration

### Environment Variables

```bash
# For GPT data generation
export OPENAI_API_KEY="your-key-here"

# For GPU training
export CUDA_VISIBLE_DEVICES=0

# For Hugging Face models
export ENABLE_HF_LLM=1
```

### Data Requirements

**Training Data Format:**
```json
[
  {
    "text": "example text",
    "labels": {
      "neutral": 0,
      "stress": 1,
      "unsafe_environment": 0,
      "emotional_distress": 0,
      "self_harm_low": 0,
      "self_harm_high": 0
    }
  }
]
```

**Files Needed:**
- `train_data.json`: Training examples
- `val_data.json`: Validation examples
- `feedback_data.json`: Human feedback (for RLHF)

## üìä Expected Results

### Baseline (Simple BERT)
- Accuracy: 75-80%
- F1 Score: 0.70-0.75

### After Advanced DL Training
- Accuracy: 85-90%
- F1 Score: 0.80-0.85
- **Improvement**: +10-15%

### After RLHF Fine-tuning
- Accuracy: 88-92%
- F1 Score: 0.85-0.88
- **Improvement**: +3-5%

### After Ensemble
- Accuracy: 90-95%
- F1 Score: 0.88-0.92
- **Improvement**: +2-5%

### After Full Pipeline
- Accuracy: **95-98%**
- F1 Score: **0.92-0.96**
- **Total Improvement**: +20-30%

## üéì Advanced Techniques Explained

### 1. LLM Data Generation

**Why**: Real-world data is limited and expensive to collect

**How**: Use GPT/LLM to generate diverse examples that:
- Cover edge cases
- Include rare patterns
- Maintain quality
- Scale easily

**Impact**: +5-10% accuracy from better data coverage

### 2. Multi-Architecture Training

**Why**: Different architectures capture different patterns

**How**: Train BERT, RoBERTa, DeBERTa separately, then ensemble

**Impact**: +3-5% accuracy from model diversity

### 3. RLHF

**Why**: Human feedback captures nuances models miss

**How**: Fine-tune on feedback signals to align with human judgment

**Impact**: +2-4% accuracy from human alignment

### 4. Focal Loss

**Why**: Class imbalance (few self-harm examples vs many neutral)

**How**: Focus learning on hard examples

**Impact**: +5-8% F1 for minority classes

### 5. Attention Pooling

**Why**: CLS token may miss important information

**How**: Learn which tokens matter most

**Impact**: +2-4% overall accuracy

### 6. Mixed Precision Training

**Why**: Faster training, same accuracy

**How**: Use FP16 for computation, FP32 for critical operations

**Impact**: 2x speedup, minimal accuracy loss

## üìà Monitoring Training

### Metrics to Watch

1. **Training Loss**: Should decrease steadily
2. **Validation F1**: Should increase, then plateau
3. **Per-Class F1**: Check minority classes (self_harm_high, etc.)
4. **Calibration**: Probabilities should match actual frequencies

### Early Stopping

- Stop if validation F1 doesn't improve for 5 epochs
- Prevents overfitting
- Saves training time

### Checkpointing

- Models saved after each epoch
- Best model (by F1) automatically saved
- Can resume training from checkpoint

## üêõ Troubleshooting

### Issue: Out of Memory

**Solution**:
- Reduce batch size (16 ‚Üí 8)
- Use gradient accumulation
- Enable mixed precision
- Use smaller model (bert-base ‚Üí distilbert)

### Issue: Slow Training

**Solution**:
- Enable mixed precision
- Use GPU (CUDA)
- Reduce sequence length (128 ‚Üí 64)
- Use smaller model

### Issue: Low Accuracy

**Solution**:
- Check data quality
- Increase training data
- Try different architectures
- Adjust hyperparameters
- Use ensemble

### Issue: Overfitting

**Solution**:
- Increase dropout
- Add weight decay
- Use early stopping
- Get more training data
- Use data augmentation

## ‚úÖ Checklist

Before training:
- [ ] Install all dependencies (`pip install -r requirements.txt`)
- [ ] Prepare training data (`train_data.json`, `val_data.json`)
- [ ] Set up GPU (if available)
- [ ] Set OpenAI API key (optional, for GPT generation)
- [ ] Check disk space (models can be large)

During training:
- [ ] Monitor training loss
- [ ] Check validation metrics
- [ ] Watch for overfitting
- [ ] Save checkpoints regularly

After training:
- [ ] Evaluate on test set
- [ ] Compare with baseline
- [ ] Test on edge cases
- [ ] Update production model
- [ ] Document results

## üöÄ Next Steps

1. **Run the pipeline**: `python ultimate_training_pipeline.py`
2. **Evaluate results**: Test on validation set
3. **Deploy**: Update `multistage_classifier.py` to use new models
4. **Monitor**: Track performance in production
5. **Iterate**: Collect feedback, retrain, improve

## üìö References

- **BERT**: [Devlin et al., 2019](https://arxiv.org/abs/1810.04805)
- **RoBERTa**: [Liu et al., 2019](https://arxiv.org/abs/1907.11692)
- **DeBERTa**: [He et al., 2021](https://arxiv.org/abs/2006.03654)
- **RLHF**: [Ouyang et al., 2022](https://arxiv.org/abs/2203.02155)
- **Focal Loss**: [Lin et al., 2017](https://arxiv.org/abs/1708.02002)

---

**Ready to achieve 98%+ accuracy? Run `python ultimate_training_pipeline.py`!** üöÄ

